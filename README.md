В данном репозитории приложено решение в рамках контеста от Kazan Express (https://kazanexpress.ru/about).
Выложенное решение заняло 3-е место (мои последние 4 цифры номера телефона - 7880) среди всех участников (рэнкинг содержит первых 50 участников, хотя есть подозрение, что участников было больше - в группе Telegram по контесту присутствует около 300 человек).

### Задание
У каждого товара есть:

- *id* - идентификатор товара
- *title - заголовок*
- *short_description - краткое описание*
- *name_value_characteristics - название:значение* характеристики товара, может быть несколько для одного товара и для одной характеристик. Пример: `name1: value1 | value2 | valueN_1 / name2: value1 | value2 | valueN_2 / nameK: value1 | value2 | valueN_K`
- *rating - средний рейтинг товара*
- *feedback_quantity - количество отзывов по товару*
- *category_id - категория товара(таргет)*

Нужно предсказать категории (category_id) для переменных из файла test.parquet, используя для тренировки данные из train.parquet.
Тренировка проходит на ок. 250 тыс. размеченных наименований товаров, в тестовой выборке ок. 70 тыс. товаров, категорию которых нам нужно определить.

Финальный результат оценивался по иерархическому f1-скору, который учитывает не только правильность финальной категории товара, но и правильность пути до этой финальной категории.

### Подход к решению
В решении использовался подход Local Classifier Per Node - мы спускались по классификационному древу вниз, выстраивая всю цепочку категорий для каждого товара. 
Т.к. название и описание товаров для разных классов хорошо разделимы (есть уникальные слова в рамках классов), подход Bag of Words + линейный классификатор показал себя лучше всего на кросс-валидации и на отложенной выборке.
Векторизацая переменных проводилась для каждого узла дерева с помощью TFIDF трансформера - так для каждого дочернего класса узла постоянно выделялись наиболее уникальные слова. В сочетании с TFIDF лучше всего сработал линейный SVM.

Вероятно, более сильный результат дало бы добавление итеративного тьюнинга или автоматический выбор лучшего классификатора под каждый узел дерева.
